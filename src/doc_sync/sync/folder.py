"""
Folder Sync Manager Module

Provides folder-level synchronization with concurrent file processing.
"""

import os
from datetime import datetime
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

from doc_sync import config
from doc_sync.feishu_client import FeishuClient
from doc_sync.logger import logger


from doc_sync.sync.state import SyncState

class FolderSyncManager:
    """Manages folder-level synchronization with concurrent file processing."""
    
    def __init__(self, local_root: str, cloud_root_token: str, force: bool = False, overwrite: bool = False,
                 vault_root: str = None, debug: bool = False, client: FeishuClient = None):
        self.local_root = local_root
        self.cloud_root_token = cloud_root_token
        self.force = force
        self.overwrite = overwrite
        self.vault_root = vault_root or local_root
        self.debug = debug
        self.batch_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        if client:
            self.client = client
        else:
            self.client = FeishuClient(
                config.FEISHU_APP_ID, 
                config.FEISHU_APP_SECRET, 
                user_access_token=config.FEISHU_USER_ACCESS_TOKEN
            )
        self.stats = {"created": 0, "updated": 0, "skipped": 0, "failed": 0, "deleted_cloud": 0, "deleted_local": 0}
        self._stats_lock = threading.Lock()  # Initialize early to ensure thread safety
        self.state = SyncState(self.vault_root)

    def run(self):
        """Run folder synchronization with concurrent file processing."""
        # Import here to avoid circular imports
        from doc_sync.sync.manager import SyncManager
        
        self._stats_lock = threading.Lock()
        
        logger.header(f"ÂºÄÂßãÊñá‰ª∂Â§πÂêåÊ≠•: {self.local_root} -> {self.cloud_root_token}", icon="üöÄ")
        
        sync_tasks = self._collect_sync_tasks(self.local_root, self.cloud_root_token)
        
        if not sync_tasks:
            logger.info("Ê≤°ÊúâÈúÄË¶ÅÂêåÊ≠•ÁöÑÊñá‰ª∂„ÄÇ", icon="‚úì")
            return
        
        logger.info(f"ÂèëÁé∞ {len(sync_tasks)} ‰∏™‰ªªÂä°Ôºå‰ΩøÁî® {config.MAX_PARALLEL_WORKERS} ‰∏™Âπ∂Ë°åÂ∑•‰ΩúÁ∫øÁ®ã...", icon="‚ö°")
        
        with ThreadPoolExecutor(max_workers=config.MAX_PARALLEL_WORKERS) as executor:
            futures = {executor.submit(self._execute_sync_task, task, SyncManager): task for task in sync_tasks}
            
            with logger.progress(len(sync_tasks), "üîÑ ÂêåÊ≠•ËøõÂ∫¶") as update:
                for future in as_completed(futures):
                    task = futures[future]
                    try:
                        result = future.result()
                        with self._stats_lock:
                            if result == "created":
                                self.stats["created"] += 1
                            elif result == "updated":
                                self.stats["updated"] += 1
                            elif result == "deleted_cloud":
                                self.stats["deleted_cloud"] += 1
                            elif result == "failed":
                                self.stats["failed"] += 1
                    except Exception as e:
                        logger.error(f"ÂêåÊ≠•‰ªªÂä°Â§±Ë¥•: {task.get('local_path', 'unknown')}: {e}")
                        with self._stats_lock:
                            self.stats["failed"] += 1
                    finally:
                        update(1)
        
        logger.summary_table("üìä ÂêåÊ≠•Ê±áÊÄª", {
            "‚úÖ Êñ∞Â¢û/‰∏ãËΩΩ": self.stats['created'],
            "üîÑ Êõ¥Êñ∞": self.stats['updated'],
            "‚è≠Ô∏è Ë∑≥Ëøá(Êú™ÂèòÊõ¥)": self.stats['skipped'],
            "üóëÔ∏è ‰∫ëÁ´ØÂà†Èô§": self.stats['deleted_cloud'],
            "‚ùå Â§±Ë¥•": self.stats['failed']
        })

    def _collect_sync_tasks(self, local_path: str, cloud_token: str) -> List[Dict[str, Any]]:
        """Recursively collect all sync tasks from folder."""
        tasks = []
        
        try:
            local_items = os.listdir(local_path)
        except OSError as e:
            logger.error(f"Êó†Ê≥ïËØªÂèñÁõÆÂΩï {local_path}: {e}")
            return tasks
        
        cloud_files = self.client.list_folder_files(cloud_token)
        cloud_map = {f.name: f for f in cloud_files}
        used_cloud_tokens = set()

        for item in local_items:
            # Skip hidden files, attachment directories, and Obsidian-specific files
            if item.startswith('.'):
                continue
            if item in ("assets", "attachments", "_attachments"):
                continue
            # Skip Excalidraw files (they won't sync properly to Feishu)
            if item.endswith(".excalidraw") or item.endswith(".excalidraw.md"):
                continue
            # Skip canvas files
            if item.endswith(".canvas"):
                continue
            
            item_path = os.path.join(local_path, item)
            
            if os.path.isdir(item_path):
                if item in cloud_map and cloud_map[item].type == "folder":
                    used_cloud_tokens.add(cloud_map[item].token)
                    tasks.extend(self._collect_sync_tasks(item_path, cloud_map[item].token))
                else:
                    new_token = self.client.create_folder(cloud_token, item)
                    if new_token:
                        tasks.extend(self._collect_sync_tasks(item_path, new_token))
                        
            elif item.endswith(".md"):
                doc_name = item[:-3]
                if doc_name in cloud_map and cloud_map[doc_name].type == "docx":
                    token = cloud_map[doc_name].token
                    used_cloud_tokens.add(token)
                    
                    # Ê£ÄÊü•Êñá‰ª∂ÊòØÂê¶Ëá™‰∏äÊ¨°ÂêåÊ≠•ÂêéÊúâÂèòÊõ¥
                    known_info = self.state.get_by_path(item_path)
                    if known_info and not self.force and not self.overwrite:
                        last_sync_time = known_info.get("last_sync", 0)
                        current_mtime = os.path.getmtime(item_path)
                        if abs(current_mtime - last_sync_time) < 1:  # 1ÁßíÂÆπÂ∑Æ
                            # Êñá‰ª∂Êú™ÂèòÊõ¥ÔºåË∑≥Ëøá
                            with self._stats_lock:
                                self.stats["skipped"] += 1
                            continue
                    
                    tasks.append({
                        "type": "sync",
                        "local_path": item_path,
                        "doc_token": token,
                        "is_new": False
                    })
                    # Update state
                    self.state.update(item_path, token)
                else:
                    # Check if it was known before (Maybe deleted on Cloud?)
                    known_info = self.state.get_by_path(item_path)
                    if known_info:
                        # It was known, but now missing from Cloud.
                        # Decision: Re-upload (Union preference).
                        logger.info(f"Êñá‰ª∂ '{item}' Âú®‰∫ëÁ´Ø‰∏¢Â§±ÔºåÈáçÊñ∞‰∏ä‰º†...", icon="üì§")
                        pass
                    
                    new_token = self.client.create_docx(cloud_token, doc_name)
                    if new_token:
                        tasks.append({
                            "type": "sync",
                            "local_path": item_path,
                            "doc_token": new_token,
                            "is_new": True
                        })
                        self.state.update(item_path, new_token)
                    else:
                        with self._stats_lock:
                            self.stats["failed"] += 1
        
        # Check cloud files
        for name, file in cloud_map.items():
            if file.token not in used_cloud_tokens:
                if name in ["DocSync_Assets", "assets", ".Trash"]:
                    continue
                
                # Check if this file was known in our state
                known_info = self.state.get_by_token(file.token)
                
                if known_info:
                    # It was in our state, but NOT found in local scan (otherwise it would be in used_cloud_tokens).
                    # This means user DELETED it locally.
                    # Action: Delete from Cloud.
                    logger.info(f"Ê£ÄÊµãÂà∞Êú¨Âú∞Âà†Èô§: '{name}'ÔºåÊ≠£Âú®ÂêåÊ≠•Âà†Èô§‰∫ëÁ´ØÊñá‰ª∂...", icon="üóëÔ∏è")
                    
                    # Reconstruct absolute local path from state
                    rel_path = self.state.token_map.get(file.token, name)
                    local_abs_path = os.path.join(self.vault_root, rel_path)
                    
                    tasks.append({
                        "type": "delete_cloud",
                        "doc_token": file.token,
                        "file_type": file.type,
                        "local_path": local_abs_path
                    })
                    continue
                
                # Not in state -> New on Cloud -> Download
                if file.type == "docx":
                    local_name = name + ".md" 
                    local_file_path = os.path.join(local_path, local_name)
                    
                    logger.info(f"ÂèëÁé∞‰∫ëÁ´ØÊñ∞Â¢ûÊñáÊ°£: '{name}'ÔºåÂáÜÂ§á‰∏ãËΩΩ...", icon="üì•")
                    tasks.append({
                        "type": "sync",
                        "local_path": local_file_path,
                        "doc_token": file.token,
                        "is_new": False 
                    })
                    # We will update state after successful download in execute

                elif file.type == "folder":
                    local_folder_path = os.path.join(local_path, name)
                    if not os.path.exists(local_folder_path):
                        os.makedirs(local_folder_path)
                        logger.info(f"ÂèëÁé∞‰∫ëÁ´ØÊñ∞Â¢ûÊñá‰ª∂Â§π: '{name}'ÔºåÂ∑≤ÂàõÂª∫Êú¨Âú∞ÁõÆÂΩï", icon="üìÇ")
                    
                    tasks.extend(self._collect_sync_tasks(local_folder_path, file.token))
                
                else:
                    logger.info(f"Ë∑≥Ëøá‰∫ëÁ´ØÈùûÊñáÊ°£Êñá‰ª∂: '{name}' ({file.type})", icon="‚è≠Ô∏è")

        return tasks

    def _execute_sync_task(self, task: Dict[str, Any], SyncManager) -> str:
        """Execute a single sync task."""
        try:
            task_type = task.get("type", "sync")
            
            if task_type == "delete_cloud":
                self.client.delete_file(task["doc_token"], file_type=task.get("file_type", "docx"))
                
                # If folder, recursively remove from state
                if task.get("file_type") == "folder":
                    self.state.remove_directory(task["local_path"])
                else:
                    self.state.remove_by_token(task["doc_token"])
                    
                return "deleted_cloud"
            
            # Normal Sync
            sync = SyncManager(
                task["local_path"], 
                task["doc_token"], 
                force=self.force or task.get("is_new", False),
                overwrite=self.overwrite,
                vault_root=self.vault_root, 
                client=self.client, 
                batch_id=self.batch_id
            )
            sync.run(debug=self.debug)
            
            # Update state on success
            self.state.update(task["local_path"], task["doc_token"])
            
            return "created" if task.get("is_new") else "updated"
        except Exception as e:
            logger.error(f"‰ªªÂä°Â§±Ë¥•: {task.get('local_path', 'unknown')}: {e}")
            return "failed"

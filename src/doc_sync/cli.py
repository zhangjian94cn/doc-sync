import argparse
import json
import os
import traceback
import sys
import warnings
from typing import Optional

# Filter out deprecated pkg_resources warning from third-party libraries
warnings.filterwarnings("ignore", category=UserWarning, module='lark_oapi.ws.pb.google')

from doc_sync.sync import SyncManager, FolderSyncManager
from doc_sync.converter import MarkdownToFeishu
from doc_sync.feishu_client import FeishuClient
from doc_sync.logger import logger
from doc_sync.config import FEISHU_APP_ID, FEISHU_APP_SECRET, FEISHU_USER_ACCESS_TOKEN


def _ensure_client(user_token=None):
    """Create an authenticated FeishuClient, handling token refresh as needed."""
    if not user_token:
        user_token = FEISHU_USER_ACCESS_TOKEN
    client = FeishuClient(FEISHU_APP_ID, FEISHU_APP_SECRET, user_access_token=user_token)
    return client, user_token

def load_config(config_path: str) -> list:
    """
    Load sync tasks from configuration file.
    
    Args:
        config_path: Path to the JSON configuration file
        
    Returns:
        List of task configurations, empty list if loading fails
    """
    if not os.path.exists(config_path):
        return []
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            # Support both new dict format and old list format
            if isinstance(data, dict):
                return data.get("tasks", [])
            elif isinstance(data, list):
                return data
            return []
    except json.JSONDecodeError as e:
        logger.error(f"é…ç½®æ–‡ä»¶ JSON æ ¼å¼é”™è¯¯: {e}")
        return []
    except IOError as e:
        logger.error(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return []

def find_vault_root(path: str) -> Optional[str]:
    """
    Find the Obsidian vault root by looking for .obsidian folder upwards.
    """
    path = os.path.abspath(path)
    if os.path.isfile(path):
        path = os.path.dirname(path)
        
    current = path
    while True:
        if os.path.exists(os.path.join(current, ".obsidian")):
            return current
        parent = os.path.dirname(current)
        if parent == current: # Reached root
            return None
        current = parent

def run_single_task(local_path, cloud_token, force, overwrite=False, note="", target_folder=None, vault_root=None, debug=False, client: FeishuClient = None):
    """
    Determines whether the task is a folder or file sync and runs the appropriate manager.
    """
    if note:
        logger.header(f"å¤„ç†ä»»åŠ¡: {note}", icon="ğŸ“Œ")
    else:
        logger.header(f"å¤„ç†ä»»åŠ¡: {local_path} -> {cloud_token}", icon="ğŸ“Œ")
        
    logger.info(f"æœ¬åœ°è·¯å¾„: {local_path}", icon="ğŸ“")
    logger.info(f"äº‘ç«¯ Token: {cloud_token}", icon="â˜ï¸ ")

    # Auto-detect Vault Root if not provided
    if not vault_root:
        vault_root = find_vault_root(local_path)
        if vault_root:
             logger.info(f"è‡ªåŠ¨æ£€æµ‹åˆ° Vault Root: {vault_root}", icon="ğŸ ")

    # Ensure client
    if not client:
        client = FeishuClient(FEISHU_APP_ID, FEISHU_APP_SECRET, user_access_token=FEISHU_USER_ACCESS_TOKEN)

    if os.path.isdir(local_path):
        logger.info(f"ä»»åŠ¡ç±»å‹: æ–‡ä»¶å¤¹åŒæ­¥", icon="ğŸ“‚")
        manager = FolderSyncManager(local_path, cloud_token, force, overwrite=overwrite, vault_root=vault_root, debug=debug, client=client)
        manager.run()
    else:
        # Check if cloud_token is a folder or doc
        
        doc_token = cloud_token
        is_folder = False
        
        # Check type - Try folder first
        logger.debug(f"æ­£åœ¨æ£€æµ‹ Token ç±»å‹: {cloud_token}", icon="ğŸ”")
        file_info = client.get_file_info(cloud_token, obj_type="folder")
        
        if file_info and file_info.doc_type == "folder":
            is_folder = True
            logger.success("è¯†åˆ«ä¸ºæ–‡ä»¶å¤¹", icon="ğŸ“‚")
        else:
            # Fallback to check if it's a docx
            file_info_doc = client.get_file_info(cloud_token, obj_type="docx")
            if file_info_doc:
                logger.success(f"è¯†åˆ«ä¸ºæ–‡æ¡£ (Type: {file_info_doc.doc_type})", icon="ğŸ“„")
                is_folder = False
            else:
                logger.warning("æ— æ³•è¯†åˆ« Token ç±»å‹ï¼Œå°†å°è¯•ä½œä¸ºæ–‡æ¡£å¤„ç†...")
                is_folder = False
            
        if is_folder:
            logger.info(f"æ£€æµ‹åˆ°ç›®æ ‡ Token æ˜¯æ–‡ä»¶å¤¹ï¼Œæ­£åœ¨æŸ¥æ‰¾/åˆ›å»ºåŒåæ–‡æ¡£...", icon="ğŸ“‚")
            doc_name = os.path.basename(local_path)
            if doc_name.endswith(".md"): doc_name = doc_name[:-3]
            
            files = client.list_folder_files(cloud_token)
            target_doc = next((f for f in files if f.name == doc_name and f.type == "docx"), None)
            
            if target_doc:
                doc_token = target_doc.token
                logger.success(f"æ‰¾åˆ°ç°æœ‰æ–‡æ¡£: {doc_name} ({doc_token})", icon="âœ…")
            else:
                logger.info(f"åˆ›å»ºæ–°æ–‡æ¡£: {doc_name}", icon="ğŸ“")
                new_token = client.create_docx(cloud_token, doc_name)
                if new_token:
                    doc_token = new_token
                    force = True # Force upload for new doc
                else:
                    logger.error("åˆ›å»ºæ–‡æ¡£å¤±è´¥ï¼Œä¸­æ­¢ã€‚")
                    return

        logger.info(f"ä»»åŠ¡ç±»å‹: å•æ–‡ä»¶åŒæ­¥", icon="ğŸ“„")
        if target_folder:
            logger.info(f"ç›®æ ‡æ–‡ä»¶å¤¹: {target_folder}", icon="ğŸ“‚")
        manager = SyncManager(local_path, doc_token, force, overwrite=overwrite, vault_root=vault_root, client=client)
        manager.run(debug=debug)

def main():
    # Route to bitable subcommand if first arg is 'bitable'
    if len(sys.argv) > 1 and sys.argv[1] == "bitable":
        bitable_main()
        return
    
    parser = argparse.ArgumentParser(
        description="DocSync: åŒå‘åŒæ­¥ Obsidian (Markdown) ä¸ é£ä¹¦äº‘æ–‡æ¡£",
        formatter_class=argparse.RawTextHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  1. å•æ–‡ä»¶/æ–‡ä»¶å¤¹åŒæ­¥:
     docsync /path/to/note.md <doc_token>
     docsync /path/to/folder <folder_token>

  2. ä½¿ç”¨é…ç½®æ–‡ä»¶æ‰¹é‡åŒæ­¥ (é»˜è®¤è¯»å– sync_config.json):
     docsync

  3. å¤šç»´è¡¨æ ¼æ“ä½œ:
     docsync bitable push data.csv --app-token bascnXXX
     docsync bitable pull --app-token bascnXXX --table-id tblXXX -o output.csv

  4. è¿˜åŸå¤‡ä»½:
     docsync --restore /path/to/folder_or_file

  5. æ¸…ç†æ—§å¤‡ä»½:
     docsync --clean
"""
    )
    parser.add_argument("md_path", nargs='?', help="æœ¬åœ° Markdown æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹è·¯å¾„")
    parser.add_argument("doc_token", nargs='?', help="é£ä¹¦äº‘æ–‡æ¡£æˆ–æ–‡ä»¶å¤¹çš„ Token")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶ä¸Šä¼ ï¼ˆå³ä½¿äº‘ç«¯æ›´æ–°ï¼Œä¹Ÿä¼šè¦†ç›–äº‘ç«¯ï¼‰")
    parser.add_argument("--overwrite", action="store_true", help="å¼ºåˆ¶å…¨é‡è¦†ç›–ï¼ˆä¸è¿›è¡Œå¢é‡æ¯”å¯¹ï¼Œç›´æ¥æ¸…ç©ºäº‘ç«¯æ–‡æ¡£å¹¶é‡æ–°ä¸Šä¼ ï¼‰")
    parser.add_argument("--config", default="sync_config.json", help="æŒ‡å®šé…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: sync_config.json)")
    parser.add_argument("--vault-root", help="æ˜¾å¼æŒ‡å®š Obsidian ä»“åº“æ ¹ç›®å½• (ç”¨äºè§£æç»å¯¹è·¯å¾„çš„èµ„æºå¼•ç”¨)")
    parser.add_argument("--clean", action="store_true", help="æ¸…ç†æ¨¡å¼ï¼šé€’å½’åˆ é™¤æ‰€æœ‰å¤‡ä»½æ–‡ä»¶ (*.bak.*)")
    parser.add_argument("--restore", help="è¿˜åŸæ¨¡å¼ï¼šäº¤äº’å¼é€‰æ‹©å¹¶è¿˜åŸå¤‡ä»½ç‰ˆæœ¬")
    parser.add_argument("--debug-dump", action="store_true", help="è°ƒè¯•æ¨¡å¼ï¼šåŒæ­¥åæ‹‰å–å¹¶æ‰“å°äº‘ç«¯ç»“æ„")
    
    args = parser.parse_args()
    
    # Show help if no args provided and not using config implicitly
    if len(sys.argv) == 1 and not os.path.exists(args.config):
        parser.print_help()
        return
    
    # Mode: Restore
    if args.restore:
        from doc_sync.core.restore import run_restore_interactive
        run_restore_interactive(args.restore)
        return

    # Mode: Clean Backups
    if args.clean:
        target_path = args.md_path or "."
        # If no path arg, try to use the first local path from config
        if not args.md_path and os.path.exists(args.config):
            try:
                tasks = load_config(args.config)
                if tasks and tasks[0].get("local"):
                    target_path = tasks[0]["local"]
            except Exception as e:
                logger.debug(f"åŠ è½½é…ç½®å¤±è´¥: {e}")
                
        logger.info(f"æ­£åœ¨æ‰«æå¹¶æ¸…ç†å¤‡ä»½æ–‡ä»¶: {os.path.abspath(target_path)}")
        count = 0
        total_size = 0
        
        for root, dirs, files in os.walk(target_path):
            for file in files:
                # Match pattern: *.bak.<digits> or just *.bak
                # Standardize to handle .bak and .bak.TIMESTAMP
                if ".bak" in file:
                    # Additional check to be safe
                    is_bak = False
                    if file.endswith(".bak"):
                        is_bak = True
                    elif ".bak." in file:
                        parts = file.rsplit(".bak.", 1)
                        if len(parts) == 2 and parts[1].isdigit():
                            is_bak = True
                        elif len(parts) == 2 and "_" in parts[1]: # Handle TIMESTAMP with underscore like 20260113_094716
                             # simple check if it looks like timestamp
                             is_bak = True
                    
                    if is_bak:
                        file_path = os.path.join(root, file)
                        try:
                            s = os.path.getsize(file_path)
                            os.remove(file_path)
                            logger.info(f"  åˆ é™¤: {file}")
                            count += 1
                            total_size += s
                        except Exception as e:
                            logger.error(f"  åˆ é™¤å¤±è´¥ {file}: {e}")
        
        logger.success(f"æ¸…ç†å®Œæˆã€‚å…±åˆ é™¤ {count} ä¸ªæ–‡ä»¶ï¼Œé‡Šæ”¾ {total_size/1024:.2f} KBã€‚")
        return

    # Check Auth and Login if needed
    user_token = FEISHU_USER_ACCESS_TOKEN
    
    # Init Client (Temporary for validation)
    client = FeishuClient(FEISHU_APP_ID, FEISHU_APP_SECRET, user_access_token=user_token)
    
    # Validate and Auto-Refresh Token
    if user_token:
        # logger.debug("æ£€æŸ¥ Token æœ‰æ•ˆæ€§...")
        try:
            from lark_oapi.api.authen.v1.model import GetUserInfoRequest
            req = GetUserInfoRequest.builder().build()
            # We need to construct request option manually or use client's internal helper if exposed
            # FeishuClient._get_request_option is protected but accessible
            opt = client._get_request_option()
            
            resp = client.client.authen.v1.user_info.get(req, opt)
            if not resp.success():
                # 99991677: Token Expired
                # 20005: Invalid Access Token (e.g. revoked or malformed)
                if resp.code == 99991677 or resp.code == 20005: 
                    logger.warning(f"Token å¤±æ•ˆ (Code: {resp.code})ï¼Œå°è¯•è‡ªåŠ¨åˆ·æ–°...")
                    from doc_sync.core.auth import FeishuAuthenticator
                    auth = FeishuAuthenticator()
                    new_token = auth.refresh()
                    if new_token:
                        user_token = new_token
                        from doc_sync import config as src_config
                        src_config.FEISHU_USER_ACCESS_TOKEN = new_token
                        # Re-init Client with new token
                        client = FeishuClient(FEISHU_APP_ID, FEISHU_APP_SECRET, user_access_token=user_token)
                        logger.success("Token è‡ªåŠ¨åˆ·æ–°æˆåŠŸ")
                    else:
                        logger.warning("Refresh Token å·²è¿‡æœŸï¼Œæ­£åœ¨è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨é‡æ–°ç™»å½•...")
                        new_token = auth.login()
                        if new_token:
                            user_token = new_token
                            from doc_sync import config as src_config
                            src_config.FEISHU_USER_ACCESS_TOKEN = new_token
                            client = FeishuClient(FEISHU_APP_ID, FEISHU_APP_SECRET, user_access_token=user_token)
                            logger.success("é‡æ–°ç™»å½•æˆåŠŸ")
                        else:
                            logger.error("ç™»å½•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æ‰‹åŠ¨é‡è¯•ã€‚")
                            sys.exit(1)
                else:
                    # Other errors (e.g. permission denied for user_info) shouldn't block main flow if token is valid?
                    # But 99991677 is specific to expiry.
                    # Let's print warning but continue, maybe sync permissions are fine.
                    logger.warning(f"Token æ ¡éªŒè­¦å‘Š: {resp.code} {resp.msg}")
        except Exception as e:
            logger.warning(f"Token æ ¡éªŒå¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()

    if not user_token and sys.stdin.isatty():
        logger.warning("æœªæ£€æµ‹åˆ° User Access Token (æ¨èç”¨äºè§£å†³æƒé™é—®é¢˜)ã€‚")
        # Check if we should prompt
        # For simplicity, let's just hint user to use setup script or auto login here?
        # Let's try auto login integration.
        try:
            choice = input("æ˜¯å¦ç«‹å³ç™»å½•é£ä¹¦ä»¥è·å–ç”¨æˆ·æƒé™? (y/n) [y]: ").lower()
            if choice in ('', 'y'):
                from doc_sync.core.auth import FeishuAuthenticator
                auth = FeishuAuthenticator()
                new_token = auth.login()
                if new_token:
                    user_token = new_token
                    # Update config module in memory is tricky if imported as from config import ...
                    # But we passed user_token to FeishuClient below, so it's fine for this run.
                    from doc_sync import config
                    config.FEISHU_USER_ACCESS_TOKEN = user_token # Update global config
        except KeyboardInterrupt:
            logger.info("\næ“ä½œå–æ¶ˆ")
            return

    # Init Client
    # Pass USER_ACCESS_TOKEN if available, otherwise it defaults to Tenant Token
    client = FeishuClient(FEISHU_APP_ID, FEISHU_APP_SECRET, user_access_token=user_token)

    # Note: We need to ensure SyncManager also uses this token.
    # SyncManager currently imports from doc_sync.config directly.
    # So we MUST update config module.
    from doc_sync import config as src_config
    src_config.FEISHU_USER_ACCESS_TOKEN = user_token
    
    # Mode 1: Single task via CLI args
    if args.md_path and args.doc_token:
        target_folder = None
        try:
            # Try to load default folder from config if available
            tasks = load_config(args.config)
            if tasks and tasks[0].get("cloud"):
                target_folder = tasks[0]["cloud"]
                logger.debug(f"è‡ªåŠ¨ä»é…ç½®ä¸­è¯»å–ç›®æ ‡æ–‡ä»¶å¤¹: {target_folder}")
        except:
            pass

        try:
            run_single_task(args.md_path, args.doc_token, args.force, overwrite=args.overwrite, note="CLI Task", target_folder=target_folder, vault_root=args.vault_root, debug=args.debug_dump, client=client)
        except Exception as e:
            logger.error(f"ä»»åŠ¡å¤±è´¥: {e}")
            traceback.print_exc()
        return

    # Mode 2: Batch sync via Config file
    logger.info(f"æœªæä¾›å‚æ•°ï¼Œæ­£åœ¨åŠ è½½é…ç½®æ–‡ä»¶: {args.config}...", icon="âš™ï¸ ")
    tasks = load_config(args.config)
    
    if not tasks:
        logger.warning(f"æœªåœ¨é…ç½®æ–‡ä»¶ä¸­æ‰¾åˆ°ä»»åŠ¡æˆ–æ–‡ä»¶ä¸å­˜åœ¨ã€‚")
        print("ç”¨æ³•: docsync <local_path> <cloud_token> [--force]")
        print("   æˆ–: docsync (ä½¿ç”¨ sync_config.json)")
        return

    success_count = 0
    total_count = 0

    for task in tasks:
        if not task.get("enabled", True):
            continue
            
        local_path = task.get("local")
        cloud_token = task.get("cloud")
        note = task.get("note", "")
        
        if not local_path or not cloud_token:
            logger.warning(f"è·³è¿‡æ— æ•ˆä»»åŠ¡: {task}")
            continue
            
        total_count += 1
        
        try:
            # Config file tasks default to non-force unless specified in json
            force_sync = args.force or task.get("force", False)
            overwrite_sync = args.overwrite or task.get("overwrite", False)
            vault_root = task.get("vault_root") or args.vault_root
            run_single_task(local_path, cloud_token, force_sync, overwrite=overwrite_sync, note=note, target_folder=task.get("target_folder"), vault_root=vault_root, debug=args.debug_dump, client=client)
            success_count += 1
        except Exception as e:
            logger.error(f"ä»»åŠ¡å¤±è´¥: {e}")
            traceback.print_exc()
            
    logger.header(f"æ‰¹é‡åŒæ­¥å®Œæˆã€‚æˆåŠŸ: {success_count}/{total_count}", icon="ğŸ")


def bitable_main():
    """CLI entry point for Bitable (å¤šç»´è¡¨æ ¼) operations."""
    parser = argparse.ArgumentParser(
        prog="docsync bitable",
        description="DocSync Bitable: åŒæ­¥æœ¬åœ°æ•°æ®ä¸é£ä¹¦å¤šç»´è¡¨æ ¼",
        formatter_class=argparse.RawTextHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  1. ä¸Šä¼  CSV åˆ°å¤šç»´è¡¨æ ¼:
     docsync bitable push data.csv --app-token bascnXXX

  2. ä»å¤šç»´è¡¨æ ¼ä¸‹è½½åˆ° CSV:
     docsync bitable pull --app-token bascnXXX --table-id tblXXX -o output.csv

  3. å¢é‡åŒæ­¥ (é»˜è®¤):
     docsync bitable push data.csv --app-token bascnXXX --table-id tblXXX --key-field "åç§°"

  4. å…¨é‡è¦†ç›–åŒæ­¥:
     docsync bitable push data.csv --app-token bascnXXX --table-id tblXXX --overwrite

  5. ä½¿ç”¨é…ç½®æ–‡ä»¶æ‰¹é‡åŒæ­¥:
     docsync bitable sync
"""
    )
    
    subparsers = parser.add_subparsers(dest="action", help="æ“ä½œç±»å‹")
    
    # Push: Local â†’ Cloud
    push_parser = subparsers.add_parser("push", help="ä¸Šä¼ æœ¬åœ°æ•°æ®åˆ°é£ä¹¦å¤šç»´è¡¨æ ¼")
    push_parser.add_argument("source", help="æœ¬åœ°æ•°æ®æ–‡ä»¶è·¯å¾„ (CSV/JSON/Markdown)")
    push_parser.add_argument("--app-token", required=True, help="å¤šç»´è¡¨æ ¼ App Token")
    push_parser.add_argument("--table-id", help="ç›®æ ‡æ•°æ®è¡¨ ID (ç•™ç©ºåˆ™è‡ªåŠ¨åˆ›å»º)")
    push_parser.add_argument("--table-name", help="æ•°æ®è¡¨åç§° (åˆ›å»ºæ–°è¡¨æ—¶ä½¿ç”¨)")
    push_parser.add_argument("--key-field", help="ç”¨äºå¢é‡åŒæ­¥çš„å”¯ä¸€æ ‡è¯†å­—æ®µå")
    push_parser.add_argument("--overwrite", action="store_true", help="å…¨é‡è¦†ç›–æ¨¡å¼ (æ¸…ç©ºåé‡æ–°ä¸Šä¼ )")
    
    # Pull: Cloud â†’ Local  
    pull_parser = subparsers.add_parser("pull", help="ä»é£ä¹¦å¤šç»´è¡¨æ ¼ä¸‹è½½æ•°æ®åˆ°æœ¬åœ°")
    pull_parser.add_argument("--app-token", required=True, help="å¤šç»´è¡¨æ ¼ App Token")
    pull_parser.add_argument("--table-id", required=True, help="æ•°æ®è¡¨ ID")
    pull_parser.add_argument("-o", "--output", required=True, help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ (CSV/JSON)")
    pull_parser.add_argument("--format", choices=["csv", "json"], help="è¾“å‡ºæ ¼å¼ (é»˜è®¤æ ¹æ®æ‰©å±•å)")
    
    # Sync: from config file
    sync_parser = subparsers.add_parser("sync", help="ä½¿ç”¨é…ç½®æ–‡ä»¶åŒæ­¥å¤šç»´è¡¨æ ¼")
    sync_parser.add_argument("--config", default="sync_config.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    # Info: show app info
    info_parser = subparsers.add_parser("info", help="æŸ¥çœ‹å¤šç»´è¡¨æ ¼ä¿¡æ¯")
    info_parser.add_argument("--app-token", required=True, help="å¤šç»´è¡¨æ ¼ App Token")
    
    args = parser.parse_args(sys.argv[2:])  # Skip 'docsync bitable'
    
    if not args.action:
        parser.print_help()
        return
    
    from doc_sync.sync.bitable_sync import BitableSyncManager
    
    client, user_token = _ensure_client()
    
    if args.action == "push":
        logger.header("å¤šç»´è¡¨æ ¼åŒæ­¥: ä¸Šä¼ ", icon="â¬†ï¸")
        logger.info(f"æ•°æ®æº: {args.source}", icon="ğŸ“„")
        
        manager = BitableSyncManager(
            client=client,
            app_token=args.app_token,
            table_id=args.table_id,
            table_name=args.table_name,
            key_field=args.key_field,
            overwrite=args.overwrite,
        )
        result = manager.push(args.source)
        logger.info(str(result))
        
    elif args.action == "pull":
        logger.header("å¤šç»´è¡¨æ ¼åŒæ­¥: ä¸‹è½½", icon="â¬‡ï¸")
        
        manager = BitableSyncManager(
            client=client,
            app_token=args.app_token,
            table_id=args.table_id,
        )
        result = manager.pull(args.output, output_format=args.format)
        logger.info(str(result))
        
    elif args.action == "sync":
        logger.header("å¤šç»´è¡¨æ ¼æ‰¹é‡åŒæ­¥", icon="ğŸ”„")
        config_path = args.config
        tasks = load_config(config_path)
        bitable_tasks = [t for t in tasks if t.get("type") == "bitable" and t.get("enabled", True)]
        
        if not bitable_tasks:
            logger.warning("é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰å¯ç”¨çš„å¤šç»´è¡¨æ ¼ä»»åŠ¡")
            return
        
        for task in bitable_tasks:
            note = task.get("note", task.get("local", "Unknown"))
            logger.header(f"å¤„ç†ä»»åŠ¡: {note}", icon="ğŸ“Œ")
            
            manager = BitableSyncManager(
                client=client,
                app_token=task["app_token"],
                table_id=task.get("table_id"),
                table_name=task.get("table_name"),
                key_field=task.get("key_field"),
                overwrite=task.get("overwrite", False),
            )
            
            direction = task.get("sync_direction", "local_to_cloud")
            if direction == "local_to_cloud":
                result = manager.push(task["local"])
            elif direction == "cloud_to_local":
                result = manager.pull(task["local"])
            else:
                logger.warning(f"ä¸æ”¯æŒçš„åŒæ­¥æ–¹å‘: {direction}")
                continue
            
            logger.info(str(result))
        
    elif args.action == "info":
        info = client.bitable_get_app_info(args.app_token)
        if info:
            logger.info(f"å¤šç»´è¡¨æ ¼: {info.get('name', 'Unknown')}")
            tables = client.bitable_list_tables(args.app_token)
            for t in tables:
                fields = client.bitable_list_fields(args.app_token, t['table_id'])
                records = client.bitable_list_records(args.app_token, t['table_id'], page_size=1)
                logger.info(f"  ğŸ“‹ {t['name']} ({t['table_id']}): {len(fields)} å­—æ®µ")
        else:
            logger.error("è·å–å¤šç»´è¡¨æ ¼ä¿¡æ¯å¤±è´¥")


if __name__ == "__main__":
    main()

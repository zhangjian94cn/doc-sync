import os
import sys
import time
import shutil
from datetime import datetime
from typing import Optional
from enum import IntEnum

import config
from src.feishu_client import FeishuClient
from src.converter import MarkdownToFeishu, FeishuToMarkdown

class SyncResult(IntEnum):
    SUCCESS = 0
    EMPTY_CLOUD = 1
    ERROR = 2

class SyncManager:
    """
    Handles synchronization of a single Markdown file with a Feishu Document.
    """
    def __init__(self, md_path: str, doc_token: str, force: bool = False, vault_root: str = None):
        self.md_path = md_path
        self.doc_token = doc_token
        self.force = force
        self.vault_root = vault_root or os.path.dirname(md_path)
        self.client = FeishuClient(
            config.FEISHU_APP_ID, 
            config.FEISHU_APP_SECRET
        )
        
    def run(self):
        """
        Main execution flow for file synchronization.
        """
        print(f"\n{'-'*30}")
        print(f"ğŸ“„ ä»»åŠ¡: {os.path.basename(self.md_path)}")
        print(f"{'-'*30}")

        if not os.path.exists(self.md_path):
            print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°æ–‡ä»¶: {self.md_path}")
            sys.exit(1)
            
        print(f"ğŸ“– è¯»å–æœ¬åœ°æ–‡ä»¶: {self.md_path}...")
        local_mtime = os.path.getmtime(self.md_path)
        print(f"ğŸ•’ æœ¬åœ°æ–‡ä»¶ä¿®æ”¹æ—¶é—´: {datetime.fromtimestamp(local_mtime)}")
        
        # Check cloud status
        print(f"ğŸ” æ­£åœ¨æ£€æŸ¥äº‘ç«¯æ–‡æ¡£çŠ¶æ€ ({self.doc_token})...")
        file_info = self.client.get_file_info(self.doc_token)
        
        should_upload = True
        
        if not file_info:
            print("âŒ é”™è¯¯: æ— æ³•è·å–äº‘ç«¯æ–‡æ¡£å…ƒæ•°æ®ã€‚")
            if not self.force:
                print("ğŸš« æ“ä½œç»ˆæ­¢ã€‚è¯·ä½¿ç”¨ --force å‚æ•°å¼ºåˆ¶ç»§ç»­ã€‚")
                sys.exit(1)
        else:
            cloud_mtime = self._parse_cloud_time(file_info.latest_modify_time)
            print(f"â˜ï¸ äº‘ç«¯æ–‡æ¡£ä¿®æ”¹æ—¶é—´: {datetime.fromtimestamp(cloud_mtime)}")
            
            if cloud_mtime > local_mtime:
                print("\nâš ï¸ è­¦å‘Š: äº‘ç«¯ç‰ˆæœ¬ æ¯” æœ¬åœ°ç‰ˆæœ¬ æ–°ï¼")
                if self.force:
                    print("ğŸ’ª å·²å¯ç”¨ --force å‚æ•°ï¼Œå°†å¼ºåˆ¶è¦†ç›–ã€‚")
                    should_upload = True
                else:
                    print("ğŸ”„ å¼€å§‹åå‘åŒæ­¥ (äº‘ç«¯ -> æœ¬åœ°)...")
                    result = self._sync_cloud_to_local()
                    if result == SyncResult.SUCCESS:
                        print("âœ… åå‘åŒæ­¥å®Œæˆã€‚")
                        should_upload = False
                    elif result == SyncResult.EMPTY_CLOUD:
                        print("â„¹ï¸ äº‘ç«¯æ–‡æ¡£ä¸ºç©ºï¼Œå‡†å¤‡ä¸Šä¼ æœ¬åœ°å†…å®¹ã€‚")
                        should_upload = True
                    else:
                        print("âŒ åå‘åŒæ­¥å¤±è´¥ï¼Œæ“ä½œç»ˆæ­¢ã€‚")
                        sys.exit(1)
            else:
                print("âœ… æœ¬åœ°ç‰ˆæœ¬è¾ƒæ–°æˆ–ä¸€è‡´ï¼Œå‡†å¤‡åŒæ­¥åˆ°äº‘ç«¯ã€‚")
                should_upload = True

        if should_upload:
            self._sync_local_to_cloud()

    def _parse_cloud_time(self, timestamp) -> float:
        """
        Heuristic to detect if timestamp is in milliseconds or seconds.
        """
        ts = int(timestamp)
        if ts > 10000000000:
            return ts / 1000.0
        return float(ts)

    def _sync_cloud_to_local(self) -> SyncResult:
        """
        Downloads cloud content and overwrites local file.
        Returns SyncResult enum.
        """
        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½äº‘ç«¯å†…å®¹å¹¶è¦†ç›–æœ¬åœ°æ–‡ä»¶: {self.md_path}...")
        
        try:
            blocks = self.client.get_all_blocks(self.doc_token)
            if not blocks:
                print("âš ï¸ è­¦å‘Š: äº‘ç«¯æ–‡æ¡£ä¸ºç©ºï¼Œæ— éœ€ä¸‹è½½ã€‚")
                return SyncResult.EMPTY_CLOUD
            
            # Define image downloader callback
            def image_downloader(token: str) -> Optional[str]:
                # Assets folder: ./assets
                assets_dir = os.path.join(os.path.dirname(self.md_path), "assets")
                filename = f"{token}.png" # Default to png
                save_path = os.path.join(assets_dir, filename)
                
                if self.client.download_image(token, save_path):
                    # Return relative path for markdown
                    return os.path.join("assets", filename)
                return None

            converter = FeishuToMarkdown(image_downloader=image_downloader)
            md_content = converter.convert(blocks)
            
            # Backup
            backup_path = f"{self.md_path}.bak.{int(time.time())}"
            shutil.copy2(self.md_path, backup_path)
            print(f"ğŸ“¦ å·²åˆ›å»ºæœ¬åœ°å¤‡ä»½: {backup_path}")
            
            with open(self.md_path, "w", encoding="utf-8") as f:
                f.write(md_content)
                
            print(f"âœ… æˆåŠŸä½¿ç”¨äº‘ç«¯å†…å®¹è¦†ç›–æœ¬åœ°æ–‡ä»¶ã€‚")
            return SyncResult.SUCCESS
            
        except Exception as e:
            print(f"âŒ åå‘åŒæ­¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return SyncResult.ERROR

    def _sync_local_to_cloud(self):
        """
        Reads local file, converts to blocks, and uploads to Feishu.
        Implements incremental sync (Diff) strategy.
        """
        import hashlib
        import difflib
        import json
        
        print("ğŸ”„ æ­£åœ¨å°† Markdown è½¬æ¢ä¸ºé£ä¹¦æ–‡æ¡£å—...")
        with open(self.md_path, "r", encoding="utf-8") as f:
            md_text = f.read()
            
        # Define image uploader callback
        def image_uploader(src: str) -> Optional[str]:
            # Resolve path strategies
            
            # 1. Check if it's already an absolute path and exists
            if os.path.isabs(src) and os.path.exists(src):
                return src
                
            # 2. Check relative to MD file (Standard Markdown)
            path_rel_md = os.path.join(os.path.dirname(self.md_path), src)
            if os.path.exists(path_rel_md):
                return path_rel_md
            
            # 3. Check relative to Vault Root (Obsidian Style)
            # If vault_root is set, try resolving from there
            if self.vault_root:
                path_rel_vault = os.path.join(self.vault_root, src)
                if os.path.exists(path_rel_vault):
                    return path_rel_vault
                
                # 4. Check in 'assets' folder in Vault Root (Common convention)
                path_assets = os.path.join(self.vault_root, "assets", os.path.basename(src))
                if os.path.exists(path_assets):
                    return path_assets
            
            print(f"âŒ å›¾ç‰‡æœªæ‰¾åˆ°: {src}")
            return None

        converter = MarkdownToFeishu(image_uploader=image_uploader)
        local_blocks = converter.parse(md_text)
        print(f"âœ¨ æœ¬åœ°å·²ç”Ÿæˆ {len(local_blocks)} ä¸ªæ–‡æ¡£å—ã€‚")
        
        # --- Incremental Sync Logic ---
        
        # 1. Fetch Cloud Blocks
        print(f"ğŸ” è·å–äº‘ç«¯ç°æœ‰å†…å®¹ä»¥è¿›è¡Œæ¯”å¯¹...")
        cloud_blocks_raw = self.client.get_all_blocks(self.doc_token)
        
        # 2. Hash Calculation Helper
        def get_block_hash(block_data, is_cloud_obj=False):
            """
            Compute a hash for block content to compare equality.
            Ignores IDs, revision info, and irrelevant styles.
            """
            # This is a simplified hashing strategy. 
            # Ideally we should canonicalize the content structure.
            # For now, we dump specific fields to JSON string and hash it.
            
            content_fingerprint = {}
            
            if is_cloud_obj:
                # Map Cloud Object to simplified dict
                b_type = block_data.block_type
                content_fingerprint["type"] = b_type
                
                # Extract content based on type
                # This mapping must match what MarkdownToFeishu produces
                attr_map = {
                    2: "text", 3: "heading1", 4: "heading2", 5: "heading3",
                    6: "heading4", 7: "heading5", 8: "heading6", 9: "heading7",
                    10: "heading8", 11: "heading9", 12: "bullet", 13: "ordered",
                    14: "code", 22: "todo", 23: "file", 27: "image"
                }
                
                attr_name = attr_map.get(b_type)
                if attr_name and hasattr(block_data, attr_name):
                    attr_obj = getattr(block_data, attr_name)
                    if attr_obj:
                        # Improved serialization for SDK objects
                        if hasattr(attr_obj, "to_dict"):
                             content_fingerprint["content"] = attr_obj.to_dict()
                        elif hasattr(attr_obj, "__dict__"):
                             # Some SDK objects might not have to_dict but have __dict__
                             # Filter private attributes
                             content_fingerprint["content"] = {k: v for k, v in attr_obj.__dict__.items() if not k.startswith('_')}
                        else:
                             # Fallback
                             content_fingerprint["content"] = str(attr_obj)
                
                # Special handling for Image Block to make it comparable
                # Cloud returns: {token: "...", width: ..., height: ...}
                # Local has: {token: "path/to/local/file", ...}
                # They will NEVER match if we compare token directly.
                if b_type == 27:
                    # We can't compare token. So we ignore token in hash?
                    # But if image CHANGED, we need to detect it.
                    # Since we can't map Cloud Token -> Local Path, we assume:
                    # If everything else matches (position in doc, maybe surrounding text?), it's the same?
                    # No, that's risky.
                    
                    # Alternative: We mark all Image blocks as "Same" (Equal) tentatively?
                    # No, then we never update images.
                    
                    # Current constraint: We CANNOT know if a cloud image matches a local image without extra metadata.
                    # Hack: For now, we EXCLUDE image token from hash. 
                    # This means: "If there is an image here, and there was an image here, we assume it's the same."
                    # This fixes the re-upload issue, BUT means changing the image file (keeping same name/location) won't trigger update.
                    # To fix THAT, user needs to delete the block or we need metadata.
                    # Let's try ignoring token for Image blocks.
                    
                    if isinstance(content_fingerprint.get("content"), dict):
                        content_fingerprint["content"].pop("token", None)
                        
            else:
                # Local Block Dict
                b_type = block_data.get("block_type")
                content_fingerprint["type"] = b_type
                
                # Extract content key
                # Keys in local_blocks are like "text", "heading1", etc.
                for k, v in block_data.items():
                    if k != "block_type" and k != "alt":
                        # Use deep copy to avoid modifying original data when popping token later
                        if isinstance(v, dict):
                            content_fingerprint["content"] = v.copy()
                        else:
                            content_fingerprint["content"] = v
                        break
                
                # Special handling for Image Block (Local)
                if b_type == 27:
                    # Remove token (path) from hash to match cloud behavior
                    if isinstance(content_fingerprint.get("content"), dict):
                        content_fingerprint["content"].pop("token", None)
            
            # Normalize: sort keys, remove None, handle objects, remove defaults
            def clean_dict(d, is_cloud=False):
                if hasattr(d, "to_dict"):
                    d = d.to_dict()
                elif hasattr(d, "__dict__"):
                    d = {k: v for k, v in d.__dict__.items() if not k.startswith('_')}

                if isinstance(d, dict):
                    new_d = {}
                    for k, v in d.items():
                        # 1. Ignore "style" field (block style, alignment, etc.)
                        if k == "style":
                            continue
                        
                        # 2. Ignore "text_element_style" if all values are false/None
                        # Or better: Recursively clean it.
                        
                        clean_v = clean_dict(v, is_cloud)
                        
                        # 3. Filter false/None values in text_element_style or general
                        if v is None:
                            continue
                        
                        # Specific logic for text_element_style to remove default false values
                        if k == "text_element_style" and isinstance(clean_v, dict):
                            # Remove keys with False values
                            clean_v = {sk: sv for sk, sv in clean_v.items() if sv}
                            if not clean_v:
                                continue # Skip empty style
                        
                        if clean_v == {} or clean_v == [] or clean_v is None:
                             # Skip empty dicts/lists? 
                             # Be careful. Local might have empty dict for some reason?
                             # Image content in local became empty dict after popping token.
                             pass

                        new_d[k] = clean_v
                    
                    # 4. Special handling for Code Block content merging
                    # Cloud splits code into lines in elements. Local has one text_run.
                    # We can't easily merge here without knowing parent type.
                    # But we can try to normalize "elements" if it's a list of text_runs.
                    
                    return new_d
                
                if isinstance(d, list):
                    return [clean_dict(x, is_cloud) for x in d]
                
                return d

            # Pre-process content to handle Code Block merging and Image emptying
            def preprocess_content(block_type, content_dict):
                # 1. Image / File: Empty it
                if block_type == 27 or block_type == 23:
                    return {}
                
                # 2. Code: Merge elements text
                if block_type == 14:
                    if "elements" in content_dict:
                        full_text = ""
                        for el in content_dict["elements"]:
                            if "text_run" in el and "content" in el["text_run"]:
                                full_text += el["text_run"]["content"]
                        return {"elements": [{"text_run": {"content": full_text}}]}
                        
                return content_dict

            clean_fp = clean_dict(content_fingerprint, is_cloud_obj)
            
            if isinstance(clean_fp.get("content"), dict):
                clean_fp["content"] = preprocess_content(clean_fp.get("type"), clean_fp["content"])
            
            # Remove empty fields from top level
            if isinstance(clean_fp, dict):
                 clean_fp = {k: v for k, v in clean_fp.items() if v}

            return hashlib.md5(json.dumps(clean_fp, sort_keys=True, default=lambda x: str(x)).encode('utf-8')).hexdigest()

        # 3. Compute Hashes
        cloud_hashes = [get_block_hash(b, is_cloud_obj=True) for b in cloud_blocks_raw]
        local_hashes = [get_block_hash(b, is_cloud_obj=False) for b in local_blocks]
        
        # 4. Calculate Diff
        sm = difflib.SequenceMatcher(None, cloud_hashes, local_hashes)
        opcodes = sm.get_opcodes()
        
        # Analysis of operations
        # opcodes: list of (tag, i1, i2, j1, j2)
        # tag: 'replace', 'delete', 'insert', 'equal'
        
        ops_count = 0
        diff_strategy_feasible = True
        
        for tag, i1, i2, j1, j2 in opcodes:
            if tag != 'equal':
                ops_count += 1
        
        # Threshold: If changes are too fragmented (> 10 chunks of changes), fallback to Full Sync
        # Because we don't have Batch Update, lots of small updates are slow.
        # Batch Insert/Delete are supported though.
        # But 'replace' = delete + insert.
        
        if ops_count == 0:
            print("âœ… æ–‡æ¡£å†…å®¹ä¸€è‡´ï¼Œæ— éœ€æ›´æ–°ã€‚")
            return

        print(f"ğŸ“Š å·®å¼‚åˆ†æ: å‘ç° {ops_count} å¤„å˜æ›´ã€‚")
        
        if ops_count > 10 or len(cloud_blocks_raw) == 0:
            print("âš ï¸ å˜æ›´è¾ƒå¤šæˆ–ä¸ºç©ºæ–‡æ¡£ï¼Œä½¿ç”¨å…¨é‡è¦†ç›–æ¨¡å¼ä»¥ç¡®ä¿é€Ÿåº¦...")
            self.client.clear_document(self.doc_token)
            self.client.add_blocks(self.doc_token, local_blocks)
        else:
            print("âš¡ï¸ ä½¿ç”¨å¢é‡åŒæ­¥æ¨¡å¼...")
            
            # Helper to pad string with display width awareness
            def pad_center(text, width):
                # Calculate display width: ASCII=1, Others(CJK)=2
                display_len = 0
                for char in text:
                    if ord(char) > 127:
                        display_len += 2
                    else:
                        display_len += 1
                
                padding = width - display_len
                if padding <= 0:
                    return text
                
                left = padding // 2
                right = padding - left
                return " " * left + text + " " * right

            # Table Header
            # Define column widths (Display Width)
            w_type = 8
            w_cloud = 12
            w_local = 12
            
            # 1. Print Diff Table (Plan)
            print(f"  â”Œ{'â”€'*w_type}â”¬{'â”€'*w_cloud}â”¬{'â”€'*w_local}â”")
            print(f"  â”‚{pad_center('ç±»å‹', w_type)}â”‚{pad_center('äº‘ç«¯å—ç´¢å¼•', w_cloud)}â”‚{pad_center('æœ¬åœ°å—ç´¢å¼•', w_local)}â”‚")
            print(f"  â”œ{'â”€'*w_type}â”¼{'â”€'*w_cloud}â”¼{'â”€'*w_local}â”¤")
            
            # Collect operations to execute
            ops_to_exec = []
            
            for tag, i1, i2, j1, j2 in reversed(opcodes):
                if tag == 'equal':
                    continue
                
                # Print readable diff
                action_map = {'delete': 'ğŸ”´ åˆ é™¤', 'insert': 'ğŸŸ¢ æ’å…¥', 'replace': 'ğŸŸ¡ æ›¿æ¢'}
                icon = action_map.get(tag, tag)
                
                # Format ranges
                c_range = f"{i1:02d}-{i2:02d}"
                l_range = f"{j1:02d}-{j2:02d}"
                
                # Print row
                print(f"  â”‚{pad_center(icon, w_type)}â”‚{pad_center(c_range, w_cloud)}â”‚{pad_center(l_range, w_local)}â”‚")
                
                ops_to_exec.append((tag, i1, i2, j1, j2))
            
            # Table Footer
            print(f"  â””{'â”€'*w_type}â”´{'â”€'*w_cloud}â”´{'â”€'*w_local}â”˜")
            
            # 2. Execute Operations
            print("ğŸš€ å¼€å§‹æ‰§è¡ŒåŒæ­¥æ“ä½œ...")
            for tag, i1, i2, j1, j2 in ops_to_exec: # Order is already reversed
                if tag == 'delete':
                    # Cloud blocks [i1:i2] need to be deleted.
                    self.client.delete_blocks_by_index(self.doc_token, i1, i2)
                    
                elif tag == 'insert':
                    # Insert local blocks [j1:j2] at cloud index i1.
                    blocks_to_insert = local_blocks[j1:j2]
                    self.client.add_blocks(self.doc_token, blocks_to_insert, index=i1)
                    
                elif tag == 'replace':
                    # Replace = Delete + Insert
                    # 1. Delete old
                    self.client.delete_blocks_by_index(self.doc_token, i1, i2)
                    # 2. Insert new at i1
                    blocks_to_insert = local_blocks[j1:j2]
                    self.client.add_blocks(self.doc_token, blocks_to_insert, index=i1)

        doc_url = f"https://feishu.cn/docx/{self.doc_token}"
        print(f"âœ… åŒæ­¥å®Œæˆï¼æ–‡æ¡£é“¾æ¥: {doc_url}")


class FolderSyncManager:
    """
    Handles recursive synchronization of a local folder with a Feishu Cloud Folder.
    """
    def __init__(self, local_root: str, cloud_root_token: str, force: bool = False, vault_root: str = None):
        self.local_root = local_root
        self.cloud_root_token = cloud_root_token
        self.force = force
        self.vault_root = vault_root or local_root
        self.client = FeishuClient(
            config.FEISHU_APP_ID, 
            config.FEISHU_APP_SECRET
        )
        
        # Stats
        self.total_files = 0
        self.processed_files = 0

        if self.cloud_root_token == "root":
            print("ğŸ” æ­£åœ¨è§£ææ ¹ç›®å½•(æˆ‘çš„ç©ºé—´) Token...")
            root_token = self.client.get_root_folder_token()
            if root_token:
                print(f"âœ… å·²è§£ææ ¹ç›®å½• Token: {root_token}")
                self.cloud_root_token = root_token
            else:
                print("âŒ é”™è¯¯: æ— æ³•è§£ææ ¹ç›®å½• Tokenã€‚")
                sys.exit(1)

    def run(self):
        """
        Main execution flow for folder synchronization.
        """
        if not os.path.exists(self.local_root):
            print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°æœ¬åœ°æ–‡ä»¶å¤¹: {self.local_root}")
            sys.exit(1)
        
        print(f"ğŸ“Š æ­£åœ¨ç»Ÿè®¡æ–‡ä»¶æ•°é‡...")
        self.total_files = self._count_files(self.local_root)
        print(f"ğŸ“¦ å‘ç° {self.total_files} ä¸ª Markdown æ–‡ä»¶ã€‚")

        print(f"ğŸš€ å¼€å§‹æ–‡ä»¶å¤¹åŒæ­¥: {self.local_root} -> {self.cloud_root_token}")
        self._sync_folder(self.local_root, self.cloud_root_token)
        print("\n" + "="*50)
        print(f"ğŸ‰ æ–‡ä»¶å¤¹åŒæ­¥å®Œæˆã€‚å…±å¤„ç† {self.processed_files}/{self.total_files} ä¸ªæ–‡ä»¶ã€‚")

    def _count_files(self, path: str) -> int:
        count = 0
        for root, dirs, files in os.walk(path):
            # Skip hidden folders
            dirs[:] = [d for d in dirs if not d.startswith('.')]
            for file in files:
                if file.endswith(".md") and not file.startswith('.'):
                    count += 1
        return count

    def _sync_folder(self, local_path: str, cloud_token: str):
        """
        Recursively syncs files and subfolders.
        """
        # 1. List cloud files in this folder
        # Only print scanning log if it's the root or we want verbose logs
        # print(f"ğŸ” æ­£åœ¨æ‰«æäº‘ç«¯æ–‡ä»¶å¤¹: {cloud_token}...")
        cloud_files = self.client.list_folder_files(cloud_token)
        # Map: name -> (token, type)
        cloud_map = {f.name: f for f in cloud_files}

        # 2. Iterate local files
        items = sorted(os.listdir(local_path))
        for item in items:
            if item.startswith('.'): continue # Skip hidden
            
            item_path = os.path.join(local_path, item)
            
            if os.path.isdir(item_path):
                # Handle Folder
                if item in cloud_map:
                    # Check if it is a folder
                    if cloud_map[item].type == "folder":
                        # print(f"ğŸ“‚ è¿›å…¥å­æ–‡ä»¶å¤¹: {item}")
                        self._sync_folder(item_path, cloud_map[item].token)
                    else:
                        print(f"âš ï¸ è­¦å‘Š: åç§°å†²çªã€‚æœ¬åœ°æ˜¯æ–‡ä»¶å¤¹ï¼Œä½†äº‘ç«¯æ˜¯ {cloud_map[item].type}ã€‚è·³è¿‡: {item}ã€‚")
                else:
                    # Create folder
                    print(f"ğŸ“ æ­£åœ¨åˆ›å»ºäº‘ç«¯æ–‡ä»¶å¤¹: {item}")
                    new_token = self.client.create_folder(cloud_token, item)
                    if new_token:
                        self._sync_folder(item_path, new_token)
            
            elif item.endswith(".md"):
                self.processed_files += 1
                doc_name = item[:-3] # Remove .md
                
                print(f"\n" + "="*50)
                print(f"ğŸ“‚ [{self.processed_files}/{self.total_files}] å¤„ç†æ–‡ä»¶: {item}")
                print("=" * 50)
                
                if doc_name in cloud_map:
                    # Sync
                    c_file = cloud_map[doc_name]
                    if c_file.type == "docx":
                        sync = SyncManager(item_path, c_file.token, self.force, vault_root=self.vault_root)
                        sync.run()
                    else:
                        print(f"âš ï¸ è­¦å‘Š: åç§°å†²çªã€‚æœ¬åœ°æ˜¯ .md æ–‡ä»¶ï¼Œä½†äº‘ç«¯æ˜¯ {c_file.type}ã€‚è·³è¿‡ã€‚")
                else:
                    # Create Doc
                    print(f"ğŸ“ æ­£åœ¨åˆ›å»ºäº‘ç«¯æ–‡æ¡£: {doc_name}")
                    new_token = self.client.create_docx(cloud_token, doc_name)
                    if new_token:
                        print(f"âœ¨ å·²åˆ›å»ºæ–‡æ¡£ {doc_name} ({new_token}), å¼€å§‹åŒæ­¥å†…å®¹...")
                        
                        # Newly created doc needs force upload to bypass timestamp check
                        sync = SyncManager(item_path, new_token, force=True, vault_root=self.vault_root)
                        sync.run()

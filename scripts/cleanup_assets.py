#!/usr/bin/env python3
"""
DocSync Assets Cleanup Script (Safe Version)

This script scans a Feishu Drive folder for duplicate/orphan files,
ensuring files still referenced by documents are NOT deleted.

Process:
1. Scan all documents in sync folder to find referenced file tokens
2. List all files in Assets folder
3. Identify orphan files (not referenced by any document)
4. Among orphans, find duplicates by filename (same name = likely duplicates)
5. Delete orphan duplicates (keeping one copy each)
"""

import os
import sys
import json
from collections import defaultdict

# Add parent directory to path for imports
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "src"))

from doc_sync.feishu_client import FeishuClient
from doc_sync.config import FEISHU_APP_ID, FEISHU_APP_SECRET, FEISHU_USER_ACCESS_TOKEN
from doc_sync.logger import logger
import lark_oapi as lark


def get_referenced_tokens_from_doc(client: FeishuClient, doc_token: str) -> set:
    """Extract all file/image tokens referenced in a document."""
    tokens = set()
    try:
        blocks = client.list_document_blocks(doc_token)
        for block in blocks:
            # Convert to dict for easier access
            try:
                block_dict = json.loads(lark.JSON.marshal(block))
            except:
                continue
            
            # Check for image blocks (type 27)
            if block_dict.get("block_type") == 27:
                img = block_dict.get("image", {})
                if img.get("token"):
                    tokens.add(img["token"])
            
            # Check for file blocks (type 23)
            elif block_dict.get("block_type") == 23:
                file_info = block_dict.get("file", {})
                if file_info.get("token"):
                    tokens.add(file_info["token"])
                    
    except Exception as e:
        logger.warning(f"æ— æ³•è¯»å–æ–‡æ¡£ {doc_token}: {e}")
    
    return tokens


def scan_docs_for_references(client: FeishuClient, folder_token: str) -> set:
    """Recursively scan all documents in a folder to find referenced tokens."""
    all_tokens = set()
    
    files = client.list_folder_files(folder_token)
    for file in files:
        if file.type == "docx":
            logger.debug(f"æ‰«ææ–‡æ¡£: {file.name}")
            tokens = get_referenced_tokens_from_doc(client, file.token)
            all_tokens.update(tokens)
        elif file.type == "folder":
            # Recursively scan subfolders
            sub_tokens = scan_docs_for_references(client, file.token)
            all_tokens.update(sub_tokens)
    
    return all_tokens


def cleanup_assets(assets_folder_token: str, docs_folder_token: str, dry_run: bool = True):
    """
    Clean up duplicate and orphan files in the assets folder.
    
    Args:
        assets_folder_token: Token of the Assets folder to clean
        docs_folder_token: Token of the documents folder (to check references)
        dry_run: If True, only report without deleting
    """
    logger.header("DocSync Assets æ¸…ç†å·¥å…· (å®‰å…¨æ¨¡å¼)", icon="ğŸ§¹")
    
    client = FeishuClient(FEISHU_APP_ID, FEISHU_APP_SECRET, 
                          user_access_token=FEISHU_USER_ACCESS_TOKEN)
    
    # Step 1: Scan all documents to find referenced tokens
    logger.info("æ­¥éª¤ 1/3: æ‰«ææ–‡æ¡£ä¸­æ­£åœ¨ä½¿ç”¨çš„é™„ä»¶...", icon="ğŸ“„")
    referenced_tokens = scan_docs_for_references(client, docs_folder_token)
    logger.success(f"å‘ç° {len(referenced_tokens)} ä¸ªæ­£åœ¨è¢«å¼•ç”¨çš„é™„ä»¶ Token")
    
    # Step 2: List all files in Assets folder
    logger.info("æ­¥éª¤ 2/3: åˆ—å‡º Assets æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶...", icon="ğŸ“‚")
    asset_files = client.list_folder_files(assets_folder_token)
    
    # Filter to actual files (not folders)
    files_only = [f for f in asset_files if f.type == "file"]
    logger.info(f"Assets æ–‡ä»¶å¤¹ä¸­å…±æœ‰ {len(files_only)} ä¸ªæ–‡ä»¶")
    
    # Step 3: Identify orphan files (not referenced)
    logger.info("æ­¥éª¤ 3/3: è¯†åˆ«å­¤ç«‹æ–‡ä»¶å’Œé‡å¤é¡¹...", icon="ğŸ”")
    
    orphan_files = []
    referenced_files = []
    
    for f in files_only:
        if f.token in referenced_tokens:
            referenced_files.append(f)
        else:
            orphan_files.append(f)
    
    logger.info(f"  - è¢«å¼•ç”¨çš„æ–‡ä»¶: {len(referenced_files)} ä¸ª (å°†ä¿ç•™)")
    logger.warning(f"  - å­¤ç«‹æ–‡ä»¶: {len(orphan_files)} ä¸ª (å¯èƒ½ä¸ºé‡å¤æˆ–æ— ç”¨)")
    
    if not orphan_files:
        logger.success("æ²¡æœ‰å­¤ç«‹æ–‡ä»¶éœ€è¦æ¸…ç†ï¼")
        return
    
    # Group orphan files by filename to find duplicates
    name_to_files = defaultdict(list)
    for f in orphan_files:
        name_to_files[f.name].append(f)
    
    # Find duplicates (same filename appearing multiple times)
    duplicate_groups = {name: files for name, files in name_to_files.items() if len(files) > 1}
    unique_orphans = {name: files[0] for name, files in name_to_files.items() if len(files) == 1}
    
    duplicate_count = sum(len(files) - 1 for files in duplicate_groups.values())
    
    logger.info(f"\nåˆ†æç»“æœ:")
    logger.info(f"  - å­¤ç«‹ä¸”å”¯ä¸€çš„æ–‡ä»¶: {len(unique_orphans)} ä¸ª")
    logger.warning(f"  - å­¤ç«‹ä¸”é‡å¤çš„æ–‡ä»¶: {duplicate_count} ä¸ª (å¯åˆ é™¤)")
    
    if not duplicate_groups:
        logger.success("æ²¡æœ‰é‡å¤çš„å­¤ç«‹æ–‡ä»¶éœ€è¦æ¸…ç†ï¼")
        return
    
    # Report duplicates
    logger.info("\né‡å¤æ–‡ä»¶è¯¦æƒ…:")
    for name, files in duplicate_groups.items():
        logger.info(f"  æ–‡ä»¶å: {name}")
        for i, f in enumerate(files):
            status = "ä¿ç•™" if i == 0 else "åˆ é™¤"
            logger.info(f"    [{status}] Token: {f.token}")
    
    if dry_run:
        logger.warning("\n[DRY RUN æ¨¡å¼] æœªæ‰§è¡Œåˆ é™¤ã€‚ä½¿ç”¨ --execute å‚æ•°æ‰§è¡Œå®é™…åˆ é™¤ã€‚")
        return
    
    # Execute deletion
    logger.header("å¼€å§‹åˆ é™¤é‡å¤çš„å­¤ç«‹æ–‡ä»¶...", icon="ğŸ—‘ï¸")
    deleted_count = 0
    failed_count = 0
    
    for name, files in duplicate_groups.items():
        for f in files[1:]:  # Keep first, delete rest
            if client.delete_file(f.token, file_type="file"):
                deleted_count += 1
                logger.success(f"å·²åˆ é™¤: {name} ({f.token})")
            else:
                failed_count += 1
                logger.error(f"åˆ é™¤å¤±è´¥: {name}")
    
    logger.success(f"\næ¸…ç†å®Œæˆï¼åˆ é™¤ {deleted_count} ä¸ªæ–‡ä»¶ï¼Œå¤±è´¥ {failed_count} ä¸ªã€‚")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="æ¸…ç†é£ä¹¦äº‘ç›˜ä¸­çš„é‡å¤/å­¤ç«‹é™„ä»¶ï¼ˆå®‰å…¨æ¨¡å¼ï¼šä¸åˆ é™¤è¢«æ–‡æ¡£å¼•ç”¨çš„æ–‡ä»¶ï¼‰",
        formatter_class=argparse.RawTextHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  1. é¢„è§ˆæ¨¡å¼ (æ¨èå…ˆè¿è¡Œ):
     python cleanup_assets.py <assets_folder_token> <docs_folder_token>
  
  2. æ‰§è¡Œåˆ é™¤:
     python cleanup_assets.py <assets_folder_token> <docs_folder_token> --execute
"""
    )
    parser.add_argument(
        "assets_token",
        help="Assets æ–‡ä»¶å¤¹çš„ Token (å­˜æ”¾é™„ä»¶çš„æ–‡ä»¶å¤¹)"
    )
    parser.add_argument(
        "docs_token",
        help="æ–‡æ¡£æ–‡ä»¶å¤¹çš„ Token (åŒæ­¥çš„ç¬”è®°æ–‡ä»¶å¤¹ï¼Œç”¨äºæ£€æŸ¥å¼•ç”¨)"
    )
    parser.add_argument(
        "--execute",
        action="store_true",
        help="æ‰§è¡Œå®é™…åˆ é™¤æ“ä½œ (é»˜è®¤åªè¿›è¡Œé¢„è§ˆ)"
    )
    
    args = parser.parse_args()
    
    cleanup_assets(args.assets_token, args.docs_token, dry_run=not args.execute)


if __name__ == "__main__":
    main()
